# Raw Data Directory

This directory contains the raw data files generated by the Reddit data pulling process.

## Files in this Directory

### 1. `submission.jsonl`
**Description**: Reddit posts from mental health related subreddits  
**Format**: JSON Lines (one JSON object per line)  
**Content**: Posts with metadata and content from r/anxiety, r/healthanxiety, r/mentalhealth, r/TrueOffMyChest

**Fields**:
- `post_id`: Unique Reddit post identifier
- `subreddit`: Subreddit name (e.g., "anxiety", "mentalhealth")
- `created_utc`: Unix timestamp of post creation
- `title`: Post title
- `selftext`: Post body text (may be empty for link posts)
- `score`: Net score (upvotes - downvotes)
- `num_comments`: Number of comments on the post
- `upvote_ratio`: Ratio of upvotes to total votes
- `over_18`: Boolean indicating if post is marked as NSFW
- `removed_by_category`: Indicates if/how post was removed

### 2. `comments_topk.jsonl`
**Description**: Top-K comments from the collected posts  
**Format**: JSON Lines (one JSON object per line)  
**Content**: Top 3 comments per post, sorted by score

**Fields**:
- `comment_id`: Unique Reddit comment identifier
- `link_id`: ID of the parent post
- `parent_id`: ID of the parent comment (for nested comments)
- `body`: Comment text content
- `created_utc`: Unix timestamp of comment creation
- `score`: Net score (upvotes - downvotes)
- `depth`: Comment nesting depth in the thread

### 3. `execution_log.md`
**Description**: Quality control and execution summary  
**Format**: Markdown  
**Content**: Detailed statistics about the data collection process

**Includes**:
- Configuration used
- Data counts and distributions
- Filter statistics (NSFW, removed, non-English)
- Time coverage information
- Quality checks (uniqueness, missingness)
- Subreddit distribution

## How to Generate These Files

### Prerequisites
1. **Reddit API Credentials**: Create a Reddit app at https://www.reddit.com/prefs/apps
2. **Environment Setup**: Create a `secret.env` file with your credentials:
   ```bash
   REDDIT_CLIENT_ID=your_client_id_here
   REDDIT_CLIENT_SECRET=your_client_secret_here
   REDDIT_USERNAME=your_reddit_username
   REDDIT_PASSWORD=your_reddit_password
   REDDIT_USER_AGENT=your_app_name:v1.0 (by u/your_username)
   ```

### Running the Data Pull
```bash
# Activate virtual environment
.venv\Scripts\activate  # Windows
# or
source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Run the data puller
python src/pull_reddit.py
```

### Configuration
Edit `configs/pull_config.yml` to customize:
- `time_range.months_back`: How many months of data to collect (default: 12)
- `subreddits`: List of subreddits to collect from
- `per_subreddit_posts`: Number of posts per subreddit (default: 1000)
- `comments.top_k`: Number of top comments per post (default: 3)
- `filters`: Content filtering options

## Data Quality

### Expected Volumes
- **Posts**: ~4,000 total (1,000 per subreddit Ã— 4 subreddits)
- **Comments**: ~12,000 total (3 comments per post on average)

### Filtering Applied
- **Language**: English only (detected using langdetect)
- **Content**: Excludes NSFW and removed posts
- **Comments**: Minimum 10 characters, excludes [removed]/[deleted]
- **Privacy**: No usernames stored, only technical IDs

### Quality Checks
- Post/comment ID uniqueness
- Time range validation
- Missing data analysis
- Filter rate monitoring

## File Formats

### JSON Lines (.jsonl)
Each line is a valid JSON object. Example:
```json
{"post_id": "abc123", "subreddit": "anxiety", "title": "Feeling overwhelmed", "score": 15}
{"post_id": "def456", "subreddit": "mentalhealth", "title": "Need advice", "score": 8}
```

### Reading the Files
```python
import json

# Read posts
with open('data/raw/submission.jsonl', 'r', encoding='utf-8') as f:
    posts = [json.loads(line) for line in f]

# Read comments
with open('data/raw/comments_topk.jsonl', 'r', encoding='utf-8') as f:
    comments = [json.loads(line) for line in f]
```

### Logs
Check `execution_log.md` for detailed information about:
- How many posts were filtered and why
- Time coverage of collected data
- Quality metrics and potential issues

## Data Usage

This raw data is intended for:
- Mental health research and analysis
- Sentiment analysis and topic modeling
- Weak labeling for machine learning models
- Academic research on social media and mental health

**Note**: This data contains sensitive mental health content. Use responsibly and in accordance with ethical guidelines for social media research.

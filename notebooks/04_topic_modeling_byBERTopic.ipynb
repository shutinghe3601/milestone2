{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b342df",
   "metadata": {},
   "source": [
    "[← Previous: Topic Modeling with NMF ](03_topic_modeling_byNMF.ipynb)\n",
    "# **Unsupervised Topic Modeling with BERTopic**\n",
    "\n",
    "**Goal:** Using BERTopic model to discover topic in general Reddit communities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc55cb39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [1. Imports & Environment Settings](#1-imports--environment-settings)\n",
    "- [2. Helper Functions](#2-helper-functions)\n",
    "- [3. Data Ingestion & Preprocessing](#3-data-ingestion--preprocessing)\n",
    "- [4. Comparing Text Processing Methods](#4-comparing-text-processing-methods)\n",
    "- [5. Comparing Different Parameter Sets](#5-comparing-different-parameter-sets)\n",
    "- [6. Final Model Fitting and Output](#6-final-model-fitting--output)\n",
    "- [6. Word Could and Topic Distribution](#7-word-could-and-topic-distribution)\n",
    "\n",
    "[Next: Labeled Dataset Comparison →](05_dataset_comparison_analysis.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d44c1",
   "metadata": {},
   "source": [
    "### **Executive Summary**\n",
    "\n",
    "**Topic Modeling Performance**\n",
    "- **Final Model**: 7 distinct topics\n",
    "- **Metrics**: coherence value = 0.730, outlier fraction = 0.049, topic diversity = 97.1%\n",
    "\n",
    "**Key Findings**\n",
    "- **Overlapping themes**: Topics 0, −1, and 2 share several high-frequency words (onli, better, good), indicating they likely represent overlapping or fragmented aspects of a broader theme.\n",
    "- **Distinct clusters**: The other topics are more distinguishable, capturing unique discussions or perspectives.\n",
    "- **Topic dominance**: Topic 0 appears across nearly all subreddits and represents a large portion of posts, likely due to subreddit selection bias (overrepresentation of anxiety-related communities) rather than a modeling issue.\n",
    "- **Model validity**: The economy topic’s clear focus on Trump-market discussions shows the model can successfully separate distinct themes, supporting the interpretation that Topic 0’s size reflects dataset bias rather than poor model performance.\n",
    "\n",
    "**Limitations**\n",
    "- Our subreddit selection may introduce bias, causing the anxiety-related topic to be more widely distributed across different subreddits.\n",
    "- Overly aggressive text preprocessing could remove too many words, leaving similar terms in the documents and resulting in narrowly defined clusters.\n",
    "- BERTopic may cluster unrelated posts together or mark them as outliers. The model can also struggle with short texts due to insufficient context for embeddings to capture meaningful themes.\n",
    "- There is a trade-off between coherence, coverage, and the number of topics. To achieve higher coherence and better coverage, we reduced the number of topics. This could potentially be improved through more parameter testing, but due to time constraints, we will stop here for now.\n",
    "\n",
    "**Output Artifacts**\n",
    "- Annotated DataFrame (`data/processed/reddit_topics_by_bertopic.parquet`)\n",
    "\n",
    "Ready for downstream analysis using supervised learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff49f4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw==7.8.1 (from -r ../requirements.txt (line 2))\n",
      "  Using cached praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore==2.4.0 (from -r ../requirements.txt (line 3))\n",
      "  Using cached prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting requests==2.32.5 (from -r ../requirements.txt (line 4))\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting websocket-client==1.8.0 (from -r ../requirements.txt (line 5))\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting pandas==2.3.2 (from -r ../requirements.txt (line 8))\n",
      "  Using cached pandas-2.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting numpy==2.3.2 (from -r ../requirements.txt (line 9))\n",
      "  Using cached numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from -r ../requirements.txt (line 10)) (2.9.0.post0)\n",
      "Collecting pytz==2025.2 (from -r ../requirements.txt (line 11))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata==2025.2 (from -r ../requirements.txt (line 12))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting PyYAML==6.0.2 (from -r ../requirements.txt (line 13))\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting scikit-learn==1.7.2 (from -r ../requirements.txt (line 16))\n",
      "  Using cached scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy==1.16.2 (from -r ../requirements.txt (line 17))\n",
      "  Using cached scipy-1.16.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib==1.5.2 (from -r ../requirements.txt (line 18))\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting nltk==3.9.1 (from -r ../requirements.txt (line 21))\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langdetect==1.0.9 (from -r ../requirements.txt (line 22))\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Collecting transformers==4.56.2 (from -r ../requirements.txt (line 23))\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting matplotlib==3.10.6 (from -r ../requirements.txt (line 26))\n",
      "  Using cached matplotlib-3.10.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting seaborn==0.13.2 (from -r ../requirements.txt (line 27))\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting jupyter==1.1.1 (from -r ../requirements.txt (line 30))\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: ipykernel==6.30.1 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from -r ../requirements.txt (line 31)) (6.30.1)\n",
      "Collecting notebook==7.4.5 (from -r ../requirements.txt (line 32))\n",
      "  Using cached notebook-7.4.5-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting python-dotenv==1.1.1 (from -r ../requirements.txt (line 35))\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting openai==1.54.3 (from -r ../requirements.txt (line 36))\n",
      "  Using cached openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting update_checker>=0.18 (from praw==7.8.1->-r ../requirements.txt (line 2))\n",
      "  Using cached update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests==2.32.5->-r ../requirements.txt (line 4))\n",
      "  Using cached charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests==2.32.5->-r ../requirements.txt (line 4))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests==2.32.5->-r ../requirements.txt (line 4))\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests==2.32.5->-r ../requirements.txt (line 4))\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from python-dateutil==2.9.0.post0->-r ../requirements.txt (line 10)) (1.17.0)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.7.2->-r ../requirements.txt (line 16))\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click (from nltk==3.9.1->-r ../requirements.txt (line 21))\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk==3.9.1->-r ../requirements.txt (line 21))\n",
      "  Using cached regex-2025.9.18-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk==3.9.1->-r ../requirements.txt (line 21))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting filelock (from transformers==4.56.2->-r ../requirements.txt (line 23))\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers==4.56.2->-r ../requirements.txt (line 23))\n",
      "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from transformers==4.56.2->-r ../requirements.txt (line 23)) (25.0)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers==4.56.2->-r ../requirements.txt (line 23))\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.56.2->-r ../requirements.txt (line 23))\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.10.6->-r ../requirements.txt (line 26))\n",
      "  Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.10.6->-r ../requirements.txt (line 26))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.10.6->-r ../requirements.txt (line 26))\n",
      "  Using cached fonttools-4.60.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib==3.10.6->-r ../requirements.txt (line 26))\n",
      "  Using cached kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib==3.10.6->-r ../requirements.txt (line 26))\n",
      "  Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib==3.10.6->-r ../requirements.txt (line 26))\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting jupyter-console (from jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nbconvert (from jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting ipywidgets (from jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting jupyterlab (from jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached jupyterlab-4.4.9-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: appnope>=0.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (9.5.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (6.5.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipykernel==6.30.1->-r ../requirements.txt (line 31)) (5.14.3)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached jupyter_server-2.17.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim<0.3,>=0.2 (from notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai==1.54.3->-r ../requirements.txt (line 36))\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai==1.54.3->-r ../requirements.txt (line 36))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai==1.54.3->-r ../requirements.txt (line 36))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai==1.54.3->-r ../requirements.txt (line 36))\n",
      "  Using cached jiter-0.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.54.3->-r ../requirements.txt (line 36))\n",
      "  Using cached pydantic-2.12.0-py3-none-any.whl.metadata (83 kB)\n",
      "Collecting sniffio (from openai==1.54.3->-r ../requirements.txt (line 36))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from openai==1.54.3->-r ../requirements.txt (line 36)) (4.15.0)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.54.3->-r ../requirements.txt (line 36))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.54.3->-r ../requirements.txt (line 36))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2->-r ../requirements.txt (line 23))\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2->-r ../requirements.txt (line 23))\n",
      "  Using cached hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting jinja2>=3.0.3 (from jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached prometheus_client-0.23.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from jupyterlab->jupyter==1.1.1->-r ../requirements.txt (line 30)) (80.9.0)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.27.1->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai==1.54.3->-r ../requirements.txt (line 36))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.1 (from pydantic<3,>=1.9.0->openai==1.54.3->-r ../requirements.txt (line 36))\n",
      "  Using cached pydantic_core-2.41.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai==1.54.3->-r ../requirements.txt (line 36))\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: decorator in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (0.8.4)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached markupsafe-3.0.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached rpds_py-0.27.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (4.3.7)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached python_json_logger-4.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached mistune-3.1.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (0.7.0)\n",
      "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached lark-1.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached cffi-2.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets->jupyter==1.1.1->-r ../requirements.txt (line 30))\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.4.5->-r ../requirements.txt (line 32))\n",
      "  Using cached types_python_dateutil-2.9.0.20251008-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/lib/python3.11/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.30.1->-r ../requirements.txt (line 31)) (0.2.3)\n",
      "Using cached praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Using cached prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached pandas-2.3.2-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Using cached numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached scipy-1.16.2-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "Using cached matplotlib-3.10.6-cp311-cp311-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Using cached notebook-7.4.5-py3-none-any.whl (14.3 MB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached openai-1.54.3-py3-none-any.whl (389 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl (204 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "Using cached hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jiter-0.11.0-cp311-cp311-macosx_11_0_arm64.whl (317 kB)\n",
      "Using cached jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
      "Using cached jupyterlab-4.4.9-py3-none-any.whl (12.3 MB)\n",
      "Using cached jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Using cached pydantic-2.12.0-py3-none-any.whl (459 kB)\n",
      "Using cached pydantic_core-2.41.1-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Using cached async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl (270 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached json5-0.12.1-py3-none-any.whl (36 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
      "Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Using cached kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached markupsafe-3.0.3-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Using cached mistune-3.1.4-py3-none-any.whl (53 kB)\n",
      "Using cached bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached prometheus_client-0.23.1-py3-none-any.whl (61 kB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached python_json_logger-4.0.0-py3-none-any.whl (15 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached regex-2025.9.18-cp311-cp311-macosx_11_0_arm64.whl (286 kB)\n",
      "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Using cached lark-1.3.0-py3-none-any.whl (113 kB)\n",
      "Using cached rpds_py-0.27.1-cp311-cp311-macosx_11_0_arm64.whl (353 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Using cached webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-macosx_11_0_arm64.whl (31 kB)\n",
      "Using cached cffi-2.0.0-cp311-cp311-macosx_11_0_arm64.whl (180 kB)\n",
      "Using cached beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Using cached soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached types_python_dateutil-2.9.0.20251008-py3-none-any.whl (17 kB)\n",
      "Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, pytz, fastjsonschema, widgetsnbextension, websocket-client, webcolors, urllib3, uri-template, tzdata, typing-inspection, types-python-dateutil, tqdm, tinycss2, threadpoolctl, terminado, soupsieve, sniffio, send2trash, safetensors, rpds-py, rfc3986-validator, rfc3339-validator, regex, PyYAML, python-json-logger, python-dotenv, pyparsing, pydantic-core, pycparser, prometheus-client, pillow, pandocfilters, overrides, numpy, mistune, MarkupSafe, lark, langdetect, kiwisolver, jupyterlab_widgets, jupyterlab-pygments, jsonpointer, json5, joblib, jiter, idna, hf-xet, h11, fsspec, fqdn, fonttools, filelock, distro, defusedxml, cycler, click, charset_normalizer, certifi, bleach, babel, attrs, async-lru, annotated-types, scipy, rfc3987-syntax, requests, referencing, pydantic, pandas, nltk, jupyter-server-terminals, jinja2, httpcore, contourpy, cffi, beautifulsoup4, arrow, anyio, update_checker, scikit-learn, prawcore, matplotlib, jsonschema-specifications, isoduration, ipywidgets, huggingface-hub, httpx, argon2-cffi-bindings, tokenizers, seaborn, praw, openai, jupyter-console, jsonschema, argon2-cffi, transformers, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107/107\u001b[0m [jupyter]7\u001b[0m [notebook]104/107\u001b[0m [jupyterlab]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 PyYAML-6.0.2 annotated-types-0.7.0 anyio-4.11.0 argon2-cffi-25.1.0 argon2-cffi-bindings-25.1.0 arrow-1.3.0 async-lru-2.0.5 attrs-25.4.0 babel-2.17.0 beautifulsoup4-4.14.2 bleach-6.2.0 certifi-2025.10.5 cffi-2.0.0 charset_normalizer-3.4.3 click-8.3.0 contourpy-1.3.3 cycler-0.12.1 defusedxml-0.7.1 distro-1.9.0 fastjsonschema-2.21.2 filelock-3.20.0 fonttools-4.60.1 fqdn-1.5.1 fsspec-2025.9.0 h11-0.16.0 hf-xet-1.1.10 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.35.3 idna-3.10 ipywidgets-8.1.7 isoduration-20.11.0 jinja2-3.1.6 jiter-0.11.0 joblib-1.5.2 json5-0.12.1 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.12.0 jupyter-lsp-2.3.0 jupyter-server-2.17.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.9 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 jupyterlab_widgets-3.0.15 kiwisolver-1.4.9 langdetect-1.0.9 lark-1.3.0 matplotlib-3.10.6 mistune-3.1.4 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 nltk-3.9.1 notebook-7.4.5 notebook-shim-0.2.4 numpy-2.3.2 openai-1.54.3 overrides-7.7.0 pandas-2.3.2 pandocfilters-1.5.1 pillow-11.3.0 praw-7.8.1 prawcore-2.4.0 prometheus-client-0.23.1 pycparser-2.23 pydantic-2.12.0 pydantic-core-2.41.1 pyparsing-3.2.5 python-dotenv-1.1.1 python-json-logger-4.0.0 pytz-2025.2 referencing-0.36.2 regex-2025.9.18 requests-2.32.5 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 rpds-py-0.27.1 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 seaborn-0.13.2 send2trash-1.8.3 sniffio-1.3.1 soupsieve-2.8 terminado-0.18.1 threadpoolctl-3.6.0 tinycss2-1.4.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.56.2 types-python-dateutil-2.9.0.20251008 typing-inspection-0.4.2 tzdata-2025.2 update_checker-0.18.0 uri-template-1.3.0 urllib3-2.5.0 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec65a5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/milestone2-311/bin/python\n",
      "3.11.13 (main, Jun  5 2025, 08:21:08) [Clang 14.0.6 ]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(sys.executable)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(sys.version)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgensim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgensim.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "import gensim\n",
    "print(f\"gensim: {gensim.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36321121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f376a47a",
   "metadata": {},
   "source": [
    "## **1. Imports & Environment Settings**\n",
    "\n",
    "**Purpose**: Initialize required libraries, set reproducible seeds and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e65cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"../src\") \n",
    "\n",
    "import text_processing_functions as tp\n",
    "from typing import Sequence, Dict, Any, Tuple, List\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "\n",
    "# set environment variable to avoid tokenizer parallelism warnings\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import random\n",
    "import torch\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f8e93c",
   "metadata": {},
   "source": [
    "## **2. Helper Functions**\n",
    "\n",
    "**Purpose**: Provides utilities for text processing and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f616d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process_with_light_stop_words(texts):\n",
    "    \"\"\"\n",
    "    Generate tokens that applied light cutomized stop words for text processing method testing\n",
    "    \"\"\"\n",
    "    stop_words_light = {'<cmt>', '[deleted]', 'amp', 'http', 'https', 'edit',\n",
    "                'deleted', 'thing', 'stuff', 'really', 'just', 'like',\n",
    "                'thanks', 'lol', 'im', 'dont', 'ive', 'youre', 'thats'}\n",
    "    tokens = [\n",
    "        w\n",
    "        for w in texts\n",
    "        if w not in stop_words_light\n",
    "        and len(w) > 1\n",
    "        and \" \" not in w\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "def evaluate_bertopic(\n",
    "    model: BERTopic,\n",
    "    docs_tokens: Sequence[Sequence[str]],\n",
    "    doc_topics: Sequence[int],\n",
    "    *,\n",
    "    top_n_words: int = 10,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute BERTopic evaluation metrics:\n",
    "      • Topic Coherence (c_v) using gensim.\n",
    "      • Outlier fraction (topic == -1).\n",
    "      • Topic diversity (share of unique top words).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        Fitted BERTopic instance.\n",
    "    docs_tokens :\n",
    "        Tokenized documents used to fit/evaluate the model.\n",
    "    doc_topics :\n",
    "        Topic assignment per document (the `topics` output from fit_transform).\n",
    "    top_n_words :\n",
    "        Number of top terms per topic to use for coherence and diversity.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "        'coherence_cv' (float),\n",
    "        'outlier_fraction' (float),\n",
    "        'topic_diversity' (float)\n",
    "    \"\"\"\n",
    "    # Build gensim dictionary/corpus for coherence\n",
    "    dictionary = Dictionary(docs_tokens)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs_tokens]\n",
    "\n",
    "    # Collect top words per topic (skip the outlier topic -1)\n",
    "    topic_words: List[List[str]] = []\n",
    "    topics_dict = model.get_topics()\n",
    "    for topic_id in model.get_topic_info()[\"Topic\"]:\n",
    "        if topic_id == -1:\n",
    "            continue\n",
    "        words = [word for word, _ in topics_dict[topic_id][:top_n_words]]\n",
    "        topic_words.append(words)\n",
    "\n",
    "    # Coherence (c_v)\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topic_words,\n",
    "        texts=docs_tokens,\n",
    "        corpus=corpus,\n",
    "        dictionary=dictionary,\n",
    "        coherence=\"c_v\",\n",
    "    )\n",
    "    coherence_cv = coherence_model.get_coherence()\n",
    "\n",
    "    # Outlier fraction\n",
    "    doc_topics_arr = np.asarray(doc_topics)\n",
    "    outlier_fraction = float(np.mean(doc_topics_arr == -1))\n",
    "\n",
    "    # Topic diversity: unique words divided by total words considered\n",
    "    unique_words = len({word for words in topic_words for word in words})\n",
    "    total_words = len(topic_words) * top_n_words\n",
    "    topic_diversity = unique_words / total_words if total_words else float(\"nan\")\n",
    "    n_topics = model.get_topic_freq().query(\"Topic != -1\").shape[0]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"coherence_cv\": coherence_cv,\n",
    "        \"outlier_fraction\": outlier_fraction,\n",
    "        \"topic_diversity\": topic_diversity,\n",
    "        \"n_topics\": n_topics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3c54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "313432a2",
   "metadata": {},
   "source": [
    "## **3. Data Ingestion & Preprocessing**\n",
    "\n",
    "**Purpose**: Load the Reddit dataset, generate stop words and apply process text function imported from `src/text_processing_functions.py` to process Reddit posts.\n",
    "\n",
    "- Create a copy of the raw dataset.\n",
    "- Posts processed with the default stop_words saved as `text_all_processed`\n",
    "- Posts processed with TF-IDF-based stop words are saved as `text_all_processed_tfidf`\n",
    "- Posts processed with lightly customized stop words are saved as `text_all_processed_light`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"../data/processed/reddit_anxiety_v1.parquet\")\n",
    "raw_data = pd.read_parquet(path)\n",
    "\n",
    "# Generate two sets of stopwords: base + auto-detected\n",
    "print(\"Generating stopwords...\")\n",
    "_, stopwords = tp.process_text(\n",
    "    \"\\n\\n\".join(raw_data[\"text_all\"].astype(str)),\n",
    "    return_stopwords=True,\n",
    ")\n",
    "\n",
    "_, auto_stop, stopwords_tfidf = tp.process_text_tfidf(\n",
    "    \"\\n\\n\".join(raw_data[\"text_all\"].astype(str)),\n",
    "    return_stopwords=True,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Preprocessing text...\")\n",
    "df = raw_data.copy()\n",
    "\n",
    "df[\"text_all_processed\"] = (\n",
    "    df[\"text_all\"]\n",
    "    .fillna(\"\")\n",
    "    .apply(lambda doc: tp.process_text(doc, extra_stopwords=stopwords))\n",
    ")\n",
    "df[\"text_all_processed_tfidf\"] = (\n",
    "    df[\"text_all\"]\n",
    "    .fillna(\"\")\n",
    "    .apply(lambda doc: tp.process_text_tfidf(doc, extra_stopwords=stopwords))\n",
    ")\n",
    "\n",
    "df[\"text_all_processed_light\"] = (\n",
    "    df[\"text_all\"]\n",
    "    .fillna(\"\")\n",
    "    .apply(lambda doc: text_process_with_light_stop_words(doc.split())) )\n",
    "\n",
    "print(f\"Data preprocessing time: {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Loaded {len(df):,} submissions\")\n",
    "print(df['subreddit'].value_counts())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329ab21",
   "metadata": {},
   "source": [
    "## **4. Comparing Text Processing Methods**\n",
    "\n",
    "**Purpose**: Compare four different text processing methods using coherence, outlier fraction, topic diversity, and the number of topics as evaluation metrics to select the method that offers the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing different text processing methods\n",
    "start_time = time.time()\n",
    "\n",
    "comparing_dict = {\n",
    "    \"not using Text Processing\": df[\"text_all\"].fillna(\"\").astype(str).tolist(),\n",
    "    \"using Light Stop Words\": df[\"text_all_processed_light\"].apply(lambda toks: \" \".join(toks)).tolist(),\n",
    "    \"using Text Processing\": df[\"text_all_processed\"].apply(lambda toks: \" \".join(toks)).tolist(),\n",
    "    \"using Text Processing with TF-IDF\": df[\"text_all_processed_tfidf\"].apply(lambda toks: \" \".join(toks)).tolist(),\n",
    "}\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") \n",
    "text_process_compare = pd.DataFrame.from_dict(comparing_dict)\n",
    "metrics_list = []\n",
    "for desc, docs in comparing_dict.items():\n",
    "    print(f\"--- Processing {desc} ---\")\n",
    "    topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=UMAP(random_state=42),\n",
    "    hdbscan_model=HDBSCAN(min_cluster_size=15),\n",
    "    vectorizer_model=CountVectorizer(),\n",
    "    ctfidf_model=ClassTfidfTransformer()\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    docs_tokens = [doc.split() for doc in docs]    \n",
    "    metrics = evaluate_bertopic(topic_model, docs_tokens, topics, top_n_words=10)\n",
    "    metrics_list.append((desc, metrics))\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    [ {\"description\": desc, **metrics} for desc, metrics in metrics_list ]\n",
    ")\n",
    "print(f\"Data preprocessing time: {time.time() - start_time:.2f} seconds\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a029c1",
   "metadata": {},
   "source": [
    "### Comparing 4 text preprocessing methods for BERTopic\n",
    "**Metrics Explanation**\n",
    "- **coherence_cv**: Measuring how well the top words in each topic \"make sense together\". Higher the value means more meaningful topics.\n",
    "- **outlier_fraction**: Measuring the percentage of posts that didn't fit into any topic (HDBSCAN marked them as -1). Lower the value means better coverage.\n",
    "- **topic_diversity**: Measuing how distinct the top topic words are from each other (less duplication). Higher the value means broader, less repetitive topics.\n",
    "\n",
    "**Not using Text Process**\n",
    "- Topics read loosely (coherence ≈0.47); 14% of posts fall outside any topic; topics reuse many of the same words (diversity ≈0.50); 27 topics total.\n",
    "\n",
    "**Using Light Stop Words**\n",
    "- Small coherence bump (≈0.48) and slightly fewer outliers (13%); topics are a bit more distinct (diversity ≈0.58); 29 topics.\n",
    "\n",
    "**Using Text Process**\n",
    "- Topics are much clearer (coherence ≈0.59) and vocabulary is varied (diversity ≈0.85) with 30 topics, but almost one‑third of documents don’t land in a topic (outlier_fraction ≈0.32).\n",
    "- ✅ Recommended as baseline for improvement\n",
    "\n",
    "**Using Text Processing with TF-IDF**\n",
    "- Keeps nearly every document in a topic (outlier_fraction ≈0.01) and topics are very distinct (diversity ≈0.95), yet coherence is slightly lower than the fully processed run and the model collapses to only 4 topics.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "- Using **Text Processing** as baseline: It gives thestrong coherence and plenty of distinct topics (≈30). To tackle the higher outlier rate, tune the clustering later—relax HDBSCAN slightly or filter short/noisy docs—while keeping this richer preprocessing pipeline in place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cb67a",
   "metadata": {},
   "source": [
    "## **5. Comparing Different Parameter Sets**\n",
    "\n",
    "**Purpose**: Compare five parameter sets using coherence, outlier fraction, topic diversity, and the number of topics as evaluation metrics to select the set that offers the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing different parameters for BERTopic\n",
    "start_time = time.time()\n",
    "param_sets = [\n",
    "    { \n",
    "        \"name\": \"baseline_improvement\",\n",
    "        \"umap_model\": UMAP(n_neighbors=25, min_dist=0.1, random_state=42),\n",
    "        \"hdbscan_model\": HDBSCAN(min_cluster_size=15, min_samples=12, prediction_data=True),\n",
    "        \"vectorizer_model\": CountVectorizer(ngram_range=(1, 2), min_df=1, max_df=0.95),\n",
    "        \"ctfidf_model\": ClassTfidfTransformer(),\n",
    "        \"top_n_words\": 10\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"balanced_baseline\",\n",
    "        \"umap_model\": UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric=\"cosine\", random_state=42),\n",
    "        \"hdbscan_model\": HDBSCAN(min_cluster_size=20, min_samples=10, cluster_selection_method=\"leaf\", prediction_data=True),\n",
    "        \"vectorizer_model\": CountVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.95,\n",
    "        stop_words=None),\n",
    "        \"ctfidf_model\": ClassTfidfTransformer(reduce_frequent_words=True),\n",
    "        \"top_n_words\": 15\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"coherence_focused\",\n",
    "        \"umap_model\": UMAP(n_neighbors=30, n_components=5, min_dist=0.0, metric=\"cosine\", random_state=42),\n",
    "        \"hdbscan_model\": HDBSCAN(min_cluster_size=25, min_samples=25, cluster_selection_epsilon=0.1, cluster_selection_method=\"eom\", prediction_data=True),\n",
    "        \"vectorizer_model\": CountVectorizer(ngram_range=(1, 3), min_df=0.02,max_df=0.9,         max_features=20000, stop_words=None),\n",
    "        \"ctfidf_model\": ClassTfidfTransformer(reduce_frequent_words=False),\n",
    "        \"top_n_words\": 20\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"diversity_focused\",\n",
    "        \"umap_model\": UMAP(n_neighbors=10, n_components=7, min_dist=0.1, metric=\"cosine\", random_state=42),\n",
    "        \"hdbscan_model\": HDBSCAN(min_cluster_size=10, min_samples=5, cluster_selection_method=\"eom\", prediction_data=True),\n",
    "        \"vectorizer_model\": CountVectorizer(ngram_range=(1, 2), min_df=1, max_df=0.98, max_features=5000,stop_words=None),\n",
    "        \"ctfidf_model\": ClassTfidfTransformer(\n",
    "            reduce_frequent_words=True,\n",
    "            bm25_weighting=True,\n",
    "        ),\n",
    "        \"top_n_words\": 10,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"low_outlier\",\n",
    "        \"umap_model\": UMAP(n_neighbors=50, n_components=5, min_dist=0.0, metric=\"cosine\", random_state=42),\n",
    "        \"hdbscan_model\": HDBSCAN(min_cluster_size=30, min_samples=None, cluster_selection_method=\"leaf\", prediction_data=True),\n",
    "        \"vectorizer_model\": CountVectorizer(ngram_range=(1, 2), min_df=0.01, max_df=0.85, stop_words=None),\n",
    "        \"ctfidf_model\": ClassTfidfTransformer(),\n",
    "        \"top_n_words\": 10\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "docs = df[\"text_all_processed\"].apply(lambda toks: \" \".join(toks)).tolist()\n",
    "\n",
    "embeddings = embedding_model.encode(\n",
    "    docs,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_sets:\n",
    "    print(f\"--- Processing {params['name']} ---\")\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model, \n",
    "        umap_model=params[\"umap_model\"],\n",
    "        hdbscan_model=params[\"hdbscan_model\"],\n",
    "        vectorizer_model=params[\"vectorizer_model\"],\n",
    "        ctfidf_model=params[\"ctfidf_model\"],\n",
    "        top_n_words=params[\"top_n_words\"],\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(docs, embeddings=embeddings)\n",
    "    docs_tokens = [doc.split() for doc in docs]    \n",
    "    metrics = evaluate_bertopic(topic_model, docs_tokens, topics, top_n_words=params[\"top_n_words\"])\n",
    "    results.append({\"name\": params[\"name\"], **metrics})\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"Data preprocessing time: {time.time() - start_time:.2f} seconds\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0ba6c",
   "metadata": {},
   "source": [
    "### Comparing 5 parameter sets for BERTopic\n",
    "**Baseline Improvement**\n",
    "- Topics read very clearly (coherence ≈0.73), almost every post is assigned to a topic (outliers ≈5%), and topics use very different vocabularies (diversity ≈0.97), but the model finds only 7 topics in total.\n",
    "- ✅ Recommended as optimal parameter set for now.\n",
    "\n",
    "**Balanced Baseline**\n",
    "- Topics are moderately coherent (≈0.53), yet over half of the documents were left unassigned (outliers ≈56%); diversity stays high (≈0.94) and we get 52 topics.\n",
    "\n",
    "**Coherence Focused**\n",
    "- Coherence actually drops (≈0.45). Outliers are near zero (0.16%), and topics never reuse words (diversity = 1.0), but there are only 2 topics—too few for most analyses.\n",
    "\n",
    "**Diversity Focused**\n",
    "- Topics overlap more on wording (diversity ≈0.88) and coherence is low (≈0.49); nearly half the documents sit outside all topics (outliers ≈47%), though we can get many topics (101).\n",
    "\n",
    "**Low Outlier**\n",
    "- Topic descriptions look good (coherence ≈0.67), but 66% of documents are still marked as outliers; vocabulary stays fairly distinct (≈0.90) with 20 topics.\n",
    "\n",
    "### Conclusion\n",
    "- We are targeting high coherence value, low outlier fraction, high topic diversity and more distinct number of topics. however, none of these parameter sets hits every target at once. Only **baseline_improvement** clears the coherence ≥0.6 bar while also keeping outliers low (≈5%) and diversity high (≈0.97), but it collapses to just 7 topics. **low_outlier** has acceptable coherence, yet more than 66% of documents are flagged as outliers, so it doesn’t meet the “lower outlier” requirement. Given that, **baseline_improvement** gives the best performance, hence recommend to use the parameter set of **baseline_improvement**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552b51a",
   "metadata": {},
   "source": [
    "## **6. Final Model Fitting & Output**\n",
    "\n",
    "**Purpose**: Fit BERTopic model using optimized parameters, and output topic data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d98d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = { \n",
    "        \"name\": \"baseline_improvement\",\n",
    "        \"umap_model\": UMAP(n_neighbors=25, min_dist=0.1, random_state=42),\n",
    "        \"hdbscan_model\": HDBSCAN(min_cluster_size=15, min_samples=12, prediction_data=True),\n",
    "        \"vectorizer_model\": CountVectorizer(ngram_range=(1, 2), min_df=1, max_df=0.95),\n",
    "        \"ctfidf_model\": ClassTfidfTransformer(),\n",
    "        \"top_n_words\": 10\n",
    "    }\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model, \n",
    "    umap_model=best_params[\"umap_model\"],\n",
    "    hdbscan_model=best_params[\"hdbscan_model\"],\n",
    "    vectorizer_model=best_params[\"vectorizer_model\"],\n",
    "    ctfidf_model=best_params[\"ctfidf_model\"],\n",
    "    top_n_words=best_params[\"top_n_words\"],\n",
    ")\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings=embeddings)\n",
    "bertopic_df = topic_model.get_document_info(docs)\n",
    "bertopic_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35766c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge topic data with original dataframe and save to parquet\n",
    "bertopic_df['doc_ids'] = df.index\n",
    "merged = df.reset_index()\n",
    "merged = merged.merge(bertopic_df, how = 'left', left_on = 'index', right_on = 'doc_ids')\n",
    "merged.drop(columns=['text_all_processed_tfidf', 'text_all_processed_light', 'Document', 'doc_ids'], inplace=True)\n",
    "merged.to_parquet(\"../data/processed/reddit_topics_by_bertopic.parquet\", index=False)\n",
    "merged.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134ae1a",
   "metadata": {},
   "source": [
    "## **7. Word Could and Topic Distribution**\n",
    "\n",
    "**Purpose**: Check the topic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9192b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in merged['Name'].unique():\n",
    "    topic_docs = merged[merged['Name'] == topic]['text_all_processed']\n",
    "    # Flatten list of tokens for all docs in this topic\n",
    "    words = [word for tokens in topic_docs for word in tokens]\n",
    "    text = ' '.join(words)\n",
    "    if text.strip():  # Only plot if there are words\n",
    "        wc = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Topic {topic}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd00a5",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "- Topics 0, −1, and 2 share several common high-frequency words such as *onli, better, and good*, meaning they are likely represent similar or overlapping themes. Their shared frequent words suggest the model may have split one broader theme into several smaller clusters.\n",
    "- The remaining topics are more distinguishable, meaning those clusters capture more unique themes or user discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40798b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count topics across subreddits\n",
    "topic_subreddit_counts = merged.groupby(['subreddit', 'Name']).size().unstack(fill_value=0)\n",
    "\n",
    "# Order rows (subreddits) by total docs, and columns (topics) by total across subs\n",
    "row_order = topic_subreddit_counts.sum(axis=1).sort_values(ascending=False).index\n",
    "col_order = topic_subreddit_counts.sum(axis=0).sort_values(ascending=False).index\n",
    "ordered = topic_subreddit_counts.loc[row_order, col_order]\n",
    "\n",
    "colors = plt.get_cmap('tab20').colors\n",
    "\n",
    "# Plot as a horizontal stacked bar chart\n",
    "fig_h = max(6, 0.35 * len(ordered))  # scale height with number of subreddits\n",
    "ax = ordered.plot(kind='barh', stacked=True, figsize=(12, fig_h), color=colors)\n",
    "\n",
    "ax.set_xlabel('Number of Documents')\n",
    "ax.set_ylabel('Subreddit')\n",
    "ax.set_title('Distribution of Topics Across Subreddits')\n",
    "\n",
    "# Put largest subreddit at the top\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Cleaner legend outside\n",
    "ax.legend(title='Topic Name', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd012ee",
   "metadata": {},
   "source": [
    "### Key Findings \n",
    "- Topic 0 is widespread across all subreddits and accounts for a large proportion of the posts. This may be caused by bias in our subreddit selection that we intentionally included many anxiety-related communities, or it could indicate an issue with the model.\n",
    "- The economy topic shows a high proportion of posts discussing Trump-market–related content, which suggests that the model can distinguish between different themes. Therefore, the large size of Topic 0 is more likely due to subreddit selection bias than a modeling problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40dc485",
   "metadata": {},
   "source": [
    "[Next: Labeled Dataset Comparison →](05_dataset_comparison_analysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milestone2-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee791485",
   "metadata": {},
   "source": [
    "# 09 Â· Failure Analysis\n",
    "\n",
    "Analyze where the logistic regression classifier struggles and inspect misclassified posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68932a0b",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "- Load the production logistic regression bundle and recreate the evaluation splits.\n",
    "- Quantify performance on calibration/test data.\n",
    "- Highlight false positives/negatives and summarize their patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove -q to see installation logs\n",
    "%pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c01e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA = Path('../data')\n",
    "PROC = DATA / 'processed'\n",
    "ART = Path('../artifacts')\n",
    "\n",
    "TEXT_COL = 'text_all'\n",
    "PUNCT = r\"\"\".,!?:;()[]{}\"'/-\\\"\"\"\n",
    "TRASH = {\"[text]\", \"[image]\", \"[removed]\", \"[deleted]\"}\n",
    "KEEP_SHORT = {\"ecg\", \"sad\", \"ptsd\", \"mom\", \"dad\", \"anx\"}\n",
    "\n",
    "def tokenize(text: str):\n",
    "    tokens = []\n",
    "    for word in str(text).split():\n",
    "        w = word.strip().strip(PUNCT).lower()\n",
    "        if w and w not in TRASH and (len(w) >= 3 or w in KEEP_SHORT):\n",
    "            tokens.append(w)\n",
    "    return tokens\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "print('Setup complete')\n",
    "print(f'Data dir: {PROC.resolve()}')\n",
    "print(f'Artifacts dir: {ART.resolve()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53460b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.read_parquet(PROC / 'reddit_anxiety_v1.parquet')\n",
    "vec = joblib.load(ART / 'vec_final.joblib')\n",
    "nmf = joblib.load(ART / 'nmf_final.joblib')\n",
    "bundle = joblib.load(ART / 'triggerlens_logreg_calibrated_bundle.joblib')\n",
    "\n",
    "df_hand = pd.read_csv(PROC / 'sample_human_labels.csv')\n",
    "df_ai = pd.read_csv(PROC / 'simple_ai_labels.csv')\n",
    "\n",
    "label_sets = {}\n",
    "# Human annotations (rating >= 4)\n",
    "df_h = df_hand[['post_id']].copy()\n",
    "df_h['label'] = (pd.to_numeric(df_hand.get('anxiety_rating'), errors='coerce') >= 4).astype(int)\n",
    "label_sets['hand'] = df_h\n",
    "\n",
    "# AI labels (confidence >= 0.5, severity >= 4, anxiety/panic category)\n",
    "df_a = df_ai[['post_id']].copy()\n",
    "cat = df_ai['ai_category'].astype(str).str.lower()\n",
    "conf = pd.to_numeric(df_ai['ai_confidence'], errors='coerce').fillna(0)\n",
    "sev = pd.to_numeric(df_ai['ai_severity'], errors='coerce').fillna(0)\n",
    "df_a['label'] = (conf >= 0.5) & (sev >= 4) & (cat.str.contains('anx') | cat.str.contains('panic'))\n",
    "df_a['label'] = df_a['label'].astype(int)\n",
    "label_sets['ai'] = df_a\n",
    "\n",
    "# Combined: prefer human labels, fill gaps with AI labels\n",
    "df_h_tmp = df_h.copy(); df_h_tmp['source'] = 'hand'\n",
    "df_a_tmp = df_a.copy(); df_a_tmp['source'] = 'ai'\n",
    "df_comb = pd.concat([df_h_tmp, df_a_tmp], ignore_index=True)\n",
    "df_comb = df_comb.sort_values('source').drop_duplicates('post_id', keep='last')\n",
    "label_sets['combined'] = df_comb[['post_id', 'label']]\n",
    "\n",
    "print(f\"Posts loaded: {len(df_posts):,}\")\n",
    "print(f\"Vectorizer vocab: {getattr(vec, 'max_features', 0):,}\")\n",
    "print(f\"NMF topics: {getattr(nmf, 'n_components', 0)}\")\n",
    "print({k: len(v) for k, v in label_sets.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_scaler = bundle.get('meta_scaler')\n",
    "threshold = float(bundle.get('threshold', 0.5))\n",
    "clf = bundle['calibrated_model']\n",
    "\n",
    "def build_feature_matrix(df_subset: pd.DataFrame):\n",
    "    tokens = df_subset[TEXT_COL].fillna('').map(tokenize)\n",
    "    X_blocks = []\n",
    "    X_tfidf = vec.transform(tokens)\n",
    "    X_blocks.append(X_tfidf)\n",
    "    X_topics = nmf.transform(X_tfidf)\n",
    "    X_blocks.append(csr_matrix(X_topics))\n",
    "\n",
    "    doc_len = np.array([len(t) for t in tokens], dtype=float)[:, None]\n",
    "    has_url = (\n",
    "        df_subset[TEXT_COL]\n",
    "        .fillna('')\n",
    "        .str.contains('http', case=False)\n",
    "        .astype(int)\n",
    "        .values[:, None]\n",
    "    ).astype(float)\n",
    "    nrc = (\n",
    "        df_subset.get('anxiety_score', pd.Series(0, index=df_subset.index))\n",
    "        .fillna(0)\n",
    "        .values[:, None]\n",
    "    ).astype(float)\n",
    "    meta = np.hstack([np.log1p(doc_len), has_url, nrc])\n",
    "    scaler = meta_scaler if meta_scaler is not None else StandardScaler().fit(meta)\n",
    "    meta_scaled = scaler.transform(meta)\n",
    "\n",
    "    X_blocks.append(csr_matrix(meta_scaled))\n",
    "    X = hstack(X_blocks, format='csr')\n",
    "    X.data = np.nan_to_num(X.data, nan=0.0)\n",
    "\n",
    "    features_df = pd.DataFrame({\n",
    "        'doc_length': doc_len.ravel(),\n",
    "        'has_url': has_url.ravel(),\n",
    "        'anxiety_score': nrc.ravel(),\n",
    "    }, index=df_subset.index)\n",
    "\n",
    "    return X, tokens, features_df\n",
    "\n",
    "# Align posts with combined labels used for production\n",
    "df_labeled = df_posts.merge(label_sets['combined'], on='post_id', how='inner')\n",
    "X_all, tokens_all, meta_features = build_feature_matrix(df_labeled)\n",
    "y_all = df_labeled['label'].values\n",
    "print('Feature matrix:', X_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17918fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(X_all.shape[0])\n",
    "idx_train, idx_tmp, y_train, y_tmp = train_test_split(\n",
    "    indices, y_all, test_size=0.4, stratify=y_all, random_state=SEED\n",
    ")\n",
    "idx_cal, idx_test, y_cal, y_test = train_test_split(\n",
    "    idx_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=SEED\n",
    ")\n",
    "\n",
    "proba_all = clf.predict_proba(X_all)[:, 1]\n",
    "preds_all = (proba_all >= threshold).astype(int)\n",
    "\n",
    "df_eval = df_labeled.copy()\n",
    "df_eval['split'] = 'train'\n",
    "df_eval.loc[idx_cal, 'split'] = 'calibration'\n",
    "df_eval.loc[idx_test, 'split'] = 'test'\n",
    "df_eval['proba'] = proba_all\n",
    "df_eval['pred'] = preds_all\n",
    "df_eval['error'] = np.where(df_eval['pred'] == df_eval['label'], 'correct', 'error')\n",
    "df_eval['error_type'] = np.select(\n",
    "    [\n",
    "        (df_eval['label'] == 1) & (df_eval['pred'] == 0),\n",
    "        (df_eval['label'] == 0) & (df_eval['pred'] == 1),\n",
    "    ],\n",
    "    ['false_negative', 'false_positive'],\n",
    "    default='correct',\n",
    ")\n",
    "\n",
    "df_eval = df_eval.join(meta_features)\n",
    "df_eval['text_preview'] = df_eval[TEXT_COL].fillna('').str.slice(0, 280)\n",
    "\n",
    "print(df_eval['split'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e79624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_split(name, mask):\n",
    "    part = df_eval.loc[mask]\n",
    "    if part.empty:\n",
    "        print(f'No rows for {name}')\n",
    "        return\n",
    "    y_true = part['label'].values\n",
    "    y_hat = part['pred'].values\n",
    "    y_score = part['proba'].values\n",
    "    print(f\"\n",
    "=== {name.upper()} ({len(part)} rows) ===\")\n",
    "    print(f\"AUC: {roc_auc_score(y_true, y_score):.3f}\")\n",
    "    print(f\"Average precision: {average_precision_score(y_true, y_score):.3f}\")\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(y_true, y_hat))\n",
    "    print(classification_report(y_true, y_hat, digits=3))\n",
    "\n",
    "describe_split('Calibration', df_eval['split'] == 'calibration')\n",
    "describe_split('Test', df_eval['split'] == 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = (\n",
    "    df_eval[df_eval['split'].isin(['calibration', 'test'])]\n",
    "    .groupby('error_type')\n",
    "    .agg(\n",
    "        count=('post_id', 'size'),\n",
    "        mean_proba=('proba', 'mean'),\n",
    "        mean_doc_length=('doc_length', 'mean'),\n",
    "        url_rate=('has_url', 'mean'),\n",
    "        mean_anxiety_score=('anxiety_score', 'mean'),\n",
    "    )\n",
    "    .sort_values('count', ascending=False)\n",
    ")\n",
    "agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negs = (\n",
    "    df_eval\n",
    "    .query(\"split == 'test' and error_type == 'false_negative'\")\n",
    "    .sort_values('proba')\n",
    "    [[\n",
    "        'post_id', 'proba', 'label', 'doc_length', 'has_url', 'anxiety_score', 'text_preview'\n",
    "    ]]\n",
    "    .head(10)\n",
    ")\n",
    "false_negs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1998ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos = (\n",
    "    df_eval\n",
    "    .query(\"split == 'test' and error_type == 'false_positive'\")\n",
    "    .sort_values('proba', ascending=False)\n",
    "    [[\n",
    "        'post_id', 'proba', 'label', 'doc_length', 'has_url', 'anxiety_score', 'text_preview'\n",
    "    ]]\n",
    "    .head(10)\n",
    ")\n",
    "false_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for i, split in enumerate(['calibration', 'test']):\n",
    "    subset = df_eval[df_eval['split'] == split]\n",
    "    if subset.empty:\n",
    "        continue\n",
    "    sns.histplot(\n",
    "        subset['proba'],\n",
    "        bins=30,\n",
    "        hue=subset['label'],\n",
    "        palette='Set2',\n",
    "        element='step',\n",
    "        ax=ax[i],\n",
    "        stat='density',\n",
    "        common_norm=False,\n",
    "    )\n",
    "    ax[i].axvline(threshold, color='red', linestyle='--', label=f'Threshold={threshold:.2f}')\n",
    "    ax[i].set_title(f'Probability distribution ({split})')\n",
    "    ax[i].set_xlabel('Predicted probability')\n",
    "    ax[i].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1768d5",
   "metadata": {},
   "source": [
    "**Next steps**\n",
    "- Review the highest-confidence mistakes and annotate whether they stem from labeling noise or missing features.\n",
    "- Consider adding more metadata features (e.g., subreddit, posting hour) for systematic false positives."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

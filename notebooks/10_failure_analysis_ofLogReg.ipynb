{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Failure Analysis of Logistic Regression (TF-IDF + NMF + Meta)\n",
        "\n",
        "### Goal\n",
        "\n",
        "- Analyze failure cases (False Positives, False Negatives) of the calibrated Logistic Regression classifier to understand error patterns and improve future iterations.\n",
        "\n",
        "### Background\n",
        "\n",
        "- Model: TF-IDF + NMF + simple metadata (log token length, has_url, anxiety_score) → Logistic Regression with isotonic calibration.\n",
        "\n",
        "### Data and Artifacts\n",
        "\n",
        "- Data: `data/processed/reddit_anxiety_v1.parquet` with combined labels from `sample_human_labels.csv` (hand) and `simple_ai_labels.csv` (ai).\n",
        "- Artifacts: `artifacts/vec_final.joblib`, `artifacts/nmf_final.joblib`, and `artifacts/triggerlens_logreg_calibrated_bundle.joblib` (includes calibrated model, meta scaler, and threshold).\n",
        "\n",
        "### Method\n",
        "\n",
        "1. Build labels: hand (anxiety_rating ≥ 4) and ai (category contains anx/panic, severity ≥ 4, confidence ≥ 0.5), then combine (prefer hand).\n",
        "2. Tokenize → TF-IDF → NMF topics; add meta features; apply saved scaler.\n",
        "3. Predict with calibrated LR using saved threshold.\n",
        "4. Identify top FP (highest probability among negatives) and top FN (lowest probability among positives); list token-level contributions from TF-IDF weights (when available).\n",
        "\n",
        "### Sanity Check of This Run\n",
        "\n",
        "- Supervised samples: 1,006\n",
        "- Feature shape: (1006, 9717)\n",
        "- Positive rate: 173 / 1006 ≈ 17.2%\n",
        "- Threshold (bundle): ~0.25\n",
        "- Failures: False Positives = 120, False Negatives = 16\n",
        "  These are consistent with the reference script outputs, indicating correct loading and feature construction.\n",
        "\n",
        "### Key Results\n",
        "\n",
        "- Operating point uses threshold ≈ 0.25 (from calibration in training notebook).\n",
        "- FP = 120, FN = 16 on the combined labeled subset (N=1,006).\n",
        "- The notebook prints excerpts and token contributions for the highest-confidence FP and the highest-confidence FN.\n",
        "\n",
        "### Failure Analysis Highlights\n",
        "\n",
        "- Highest-confidence FP: Text strongly emotive/long-form ranting; top contributing tokens include generic stress/struggle terms (e.g., \"hard\", \"afford\").\n",
        "  - Observation: Lexical cues tied to hardship and apology may push probability high even in non-anxiety labels, suggesting context/sentiment overlap.\n",
        "- Highest-confidence FN: Long meta-thread content with political/religion context; high-probability positive cue n-grams present but counterweighted by strong non-anxiety tokens (e.g., \"religion\", \"god\").\n",
        "  - Observation: Topic/context tokens can dominate and suppress anxiety cues for general-discussion or meta posts.\n",
        "\n",
        "### Interpreting FP/FN Patterns\n",
        "\n",
        "- FP drivers:\n",
        "  - General distress-related lexicon (apologies, hardship, finance) without explicit anxiety context.\n",
        "  - Longer posts with many neutral n-grams where a subset of weighted tokens dominates.\n",
        "- FN drivers:\n",
        "  - Topic tokens (religion/politics) that the model has learned as non-anxiety; they can override sparse anxiety indicators.\n",
        "  - Posts with diffuse, non-self-referential language or multi-topic content.\n",
        "\n",
        "### Actionable Ideas\n",
        "\n",
        "- Features: Add character n-grams, adjust min_df/max_features; consider sublinear TF; try class-conditional n-grams.\n",
        "- Reweighting: Explore focal loss or alternative calibration/thresholds per content-type (e.g., meta threads).\n",
        "- Data: Curate more positives with subtle/implicit anxiety; add hard negatives (politics/religion discussions) to disentangle topic vs. state.\n",
        "- Modeling: Try linear+nonlinear hybrids (e.g., TF-IDF LR + small neural re-ranker) or stronger topic disentanglement.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- Combined labels have noise and potential labeling bias; moderate dataset size.\n",
        "- Linear classifier may underfit complex semantics; NMF topics can drift.\n",
        "- Current failure view is a single sample per FP/FN bucket; broader slices (e.g., top-k, per-subreddit) would provide more robust insights.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "- At the calibration-derived threshold (~0.25), recall is prioritized, yielding relatively few FNs (16) at the cost of more FPs (120).\n",
        "- FP patterns indicate hardship/sentiment overlap; FN patterns indicate topic overshadowing anxiety cues.\n",
        "- The findings support targeted feature and data improvements to reduce FPs without inflating FNs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(✓) Setup complete\n",
            "  - Data path: /Users/mariamckay/code/umich/milestone2/data/processed\n",
            "  - Artifacts path: /Users/mariamckay/code/umich/milestone2/artifacts\n"
          ]
        }
      ],
      "source": [
        "# STEP 0: Setup and Imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "\n",
        "# Compatibility shim for numpy/joblib artifacts\n",
        "if not hasattr(np, \"_core\"):\n",
        "    import numpy.core as core\n",
        "\n",
        "    sys.modules[\"numpy._core\"] = core\n",
        "    sys.modules[\"numpy._core._multiarray_umath\"] = core._multiarray_umath\n",
        "\n",
        "\n",
        "# Reusable identity for joblib pipelines\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "# Paths\n",
        "NB_DIR = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
        "ART = NB_DIR.parent / \"artifacts\"\n",
        "PROC = NB_DIR.parent / \"data\" / \"processed\"\n",
        "\n",
        "TEXT_COL = \"text_all\"\n",
        "PUNCT = \".,!?:;()[]{}\\\"'\" \"''-–—/\\\\\"\n",
        "TRASH = {\"[text]\", \"[image]\", \"[removed]\", \"[deleted]\"}\n",
        "KEEP_SHORT = {\"ecg\", \"sad\", \"ptsd\", \"mom\", \"dad\", \"anx\"}\n",
        "\n",
        "\n",
        "def tokenize(s: str):\n",
        "    tokens = []\n",
        "    for word in str(s).split():\n",
        "        w = word.strip().strip(PUNCT).lower()\n",
        "        if w and w not in TRASH and (len(w) >= 3 or w in KEEP_SHORT):\n",
        "            tokens.append(w)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "print(\"(✓) Setup complete\")\n",
        "print(f\"  - Data path: {PROC}\")\n",
        "print(f\"  - Artifacts path: {ART}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/6] Loading models...\n",
            "  - Vectorizer loaded: TfidfVectorizer\n",
            "  - Vocabulary size: 9699\n",
            "  - NMF and calibrated LR loaded\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Load models/artifacts\n",
        "print(\"[1/6] Loading models...\")\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    vec = joblib.load(ART / \"vec_final.joblib\")\n",
        "\n",
        "if not hasattr(vec, \"idf_\") or getattr(vec, \"idf_\", None) is None:\n",
        "    print(\"[ERROR] Vectorizer is not fitted properly! Please re-train models.\")\n",
        "else:\n",
        "    print(f\"  - Vectorizer loaded: {vec.__class__.__name__}\")\n",
        "    print(\n",
        "        f\"  - Vocabulary size: {len(vec.vocabulary_) if hasattr(vec, 'vocabulary_') else 'unknown'}\"\n",
        "    )\n",
        "\n",
        "nmf = joblib.load(ART / \"nmf_final.joblib\")\n",
        "bundle = joblib.load(ART / \"triggerlens_logreg_calibrated_bundle.joblib\")\n",
        "calib_model = bundle[\"calibrated_model\"]\n",
        "meta_scaler = bundle[\"meta_scaler\"]\n",
        "threshold = float(bundle.get(\"threshold\", 0.257))\n",
        "print(\"  - NMF and calibrated LR loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2/6] Loading data...\n",
            "  - Main: 6,283 rows | Hand: 599 | AI: 1,000\n"
          ]
        }
      ],
      "source": [
        "# STEP 2: Load data\n",
        "print(\"[2/6] Loading data...\")\n",
        "df_main = pd.read_parquet(PROC / \"reddit_anxiety_v1.parquet\")\n",
        "df_hand = pd.read_csv(PROC / \"sample_human_labels.csv\")\n",
        "df_ai = pd.read_csv(PROC / \"simple_ai_labels.csv\")\n",
        "print(f\"  - Main: {len(df_main):,} rows | Hand: {len(df_hand):,} | AI: {len(df_ai):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3/6] Creating labels...\n",
            "  - Supervised samples: 1,006\n"
          ]
        }
      ],
      "source": [
        "# STEP 3: Build labels (hand, ai, combined)\n",
        "print(\"[3/6] Creating labels...\")\n",
        "# hand\n",
        "df_h = df_hand[[\"post_id\"]].copy()\n",
        "rating = pd.to_numeric(df_hand.get(\"anxiety_rating\", pd.Series()), errors=\"coerce\")\n",
        "df_h[\"label\"] = (rating >= 4).astype(int)\n",
        "# ai\n",
        "cat = df_ai[\"ai_category\"].astype(str).str.lower()\n",
        "conf = pd.to_numeric(df_ai[\"ai_confidence\"], errors=\"coerce\").fillna(0)\n",
        "sev = pd.to_numeric(df_ai[\"ai_severity\"], errors=\"coerce\").fillna(0)\n",
        "df_a = df_ai[[\"post_id\"]].copy()\n",
        "df_a[\"label\"] = (\n",
        "    (cat.str.contains(\"anx\") | cat.str.contains(\"panic\")) & (sev >= 4) & (conf >= 0.5)\n",
        ").astype(int)\n",
        "# combine (prefer hand)\n",
        "df_h_temp = df_h.copy()\n",
        "df_h_temp[\"source\"] = \"hand\"\n",
        "df_a_temp = df_a.copy()\n",
        "df_a_temp[\"source\"] = \"ai\"\n",
        "df_comb = pd.concat([df_h_temp, df_a_temp], ignore_index=True)\n",
        "df_comb = df_comb.sort_values(\"source\").drop_duplicates(\"post_id\", keep=\"last\")\n",
        "label_sets = df_comb[[\"post_id\", \"label\"]]\n",
        "\n",
        "# join to main\n",
        "df_combined = df_main[df_main[\"post_id\"].isin(label_sets[\"post_id\"])].merge(\n",
        "    label_sets, on=\"post_id\", how=\"inner\"\n",
        ")\n",
        "print(f\"  - Supervised samples: {len(df_combined):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4/6] Building features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mariamckay/code/umich/milestone2/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Feature shape: (1006, 9717)\n",
            "  - Total samples: 1006\n",
            "  - Positive: 173 (17.2%)\n"
          ]
        }
      ],
      "source": [
        "# STEP 4: Build features (TF-IDF + NMF + metadata scaler)\n",
        "print(\"[4/6] Building features...\")\n",
        "\n",
        "tokens = df_combined[TEXT_COL].fillna(\"\").map(tokenize)\n",
        "\n",
        "# TF-IDF\n",
        "X_tfidf = vec.transform(tokens)\n",
        "\n",
        "# NMF topics\n",
        "W_topics = nmf.transform(X_tfidf)\n",
        "X_nmf = csr_matrix(W_topics)\n",
        "\n",
        "# Metadata\n",
        "doc_len = np.array([len(t) for t in tokens], dtype=float)[:, None]\n",
        "has_url = (\n",
        "    df_combined[TEXT_COL]\n",
        "    .fillna(\"\")\n",
        "    .str.contains(\"http\", case=False)\n",
        "    .astype(int)\n",
        "    .values[:, None]\n",
        ").astype(float)\n",
        "nrc = (\n",
        "    df_combined.get(\"anxiety_score\", pd.Series(0, index=df_combined.index))\n",
        "    .fillna(0)\n",
        "    .values[:, None]\n",
        ").astype(float)\n",
        "meta = np.hstack([np.log1p(doc_len), has_url, nrc])\n",
        "meta_scaled = meta_scaler.transform(meta)\n",
        "X_meta = csr_matrix(meta_scaled)\n",
        "\n",
        "# Combine all\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "X = hstack([X_tfidf, X_nmf, X_meta], format=\"csr\")\n",
        "X.data = np.nan_to_num(X.data, nan=0.0)\n",
        "y_true = df_combined[\"label\"].values\n",
        "texts = df_combined[TEXT_COL].fillna(\"\").tolist()\n",
        "\n",
        "print(f\"  - Feature shape: {X.shape}\")\n",
        "print(f\"  - Total samples: {len(y_true)}\")\n",
        "print(f\"  - Positive: {y_true.sum()} ({y_true.sum()/len(y_true)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5/6] Making predictions...\n",
            "\n",
            "[6/6] FAILURE ANALYSIS\n",
            "======================================================================\n",
            "Threshold: 0.250000\n",
            "Total samples: 1006\n",
            "False Positives: 120\n",
            "False Negatives: 16\n"
          ]
        }
      ],
      "source": [
        "# STEP 5: Predict\n",
        "print(\"[5/6] Making predictions...\")\n",
        "probs = calib_model.predict_proba(X)[:, 1]\n",
        "preds = (probs >= threshold).astype(int)\n",
        "\n",
        "fp_mask = (y_true == 0) & (preds == 1)\n",
        "fn_mask = (y_true == 1) & (preds == 0)\n",
        "fp_idx = np.where(fp_mask)[0]\n",
        "fn_idx = np.where(fn_mask)[0]\n",
        "\n",
        "print(\"\\n[6/6] FAILURE ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Threshold: {threshold:.6f}\")\n",
        "print(f\"Total samples: {len(y_true)}\")\n",
        "print(f\"False Positives: {len(fp_idx)}\")\n",
        "print(f\"False Negatives: {len(fn_idx)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "HIGHEST CONFIDENCE FALSE POSITIVE (FP)\n",
            "----------------------------------------------------------------------\n",
            "Index: 410\n",
            "True label: 0 (not anxiety)\n",
            "Predicted: 1 (anxiety)\n",
            "Probability: 1.0000\n",
            "\n",
            "Text excerpt (first 200 chars):\n",
            "idk how much more of this life i can take\n",
            "\n",
            "i know this will probably seem just like useless ranting but i have no one else to talk to about this. i 29f am a sahm to 5 kids. up until a few years ago th...\n",
            "\n",
            "Top anxiety-indicating tokens:\n",
            "  'you some': +0.0583\n",
            "  'hard': +0.0524\n",
            "  'contact': +0.0462\n",
            "  'sorry': +0.0414\n",
            "  'afford': +0.0395\n",
            "  'what': +0.0380\n",
            "  'few': +0.0356\n",
            "  'that you': +0.0352\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "HIGHEST CONFIDENCE FALSE NEGATIVE (FN)\n",
            "----------------------------------------------------------------------\n",
            "Index: 654\n",
            "True label: 1 (anxiety)\n",
            "Predicted: 0 (not anxiety)\n",
            "Probability: 0.0455\n",
            "\n",
            "Text excerpt (first 200 chars):\n",
            "religion mega thread\n",
            "\n",
            "please post all topics about religion here\n",
            "\n",
            "the worst religion in the world right now are people who blindly follow politicians as celebrities <cmt> humans invented the concept o...\n",
            "\n",
            "Top anxiety-indicating tokens:\n",
            "  'right now': +0.1052\n",
            "  'beyond': +0.0726\n",
            "  'because they': +0.0633\n",
            "  'politicians': +0.0575\n",
            "  'own': +0.0492\n",
            "  'being': +0.0480\n",
            "  'because': +0.0476\n",
            "  'power': +0.0446\n",
            "\n",
            "Top non-anxiety-indicating tokens:\n",
            "  'god': -0.0820\n",
            "  'who': -0.0406\n",
            "  'belief': -0.0379\n",
            "  'world': -0.0358\n",
            "  'concept': -0.0340\n",
            "  'religion': -0.0324\n",
            "  'edit': -0.0250\n",
            "  'humans': -0.0235\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# STEP 6: Inspect top FP and FN with token contributions\n",
        "# Try to extract underlying LR coef for TF-IDF feature contributions\n",
        "coef = None\n",
        "try:\n",
        "    base_clf = calib_model.calibrated_classifiers_[0].estimator\n",
        "    coef = (\n",
        "        base_clf.coef_[0]\n",
        "        if hasattr(base_clf.coef_, \"shape\") and base_clf.coef_.ndim == 2\n",
        "        else base_clf.coef_\n",
        "    )\n",
        "except Exception:\n",
        "    print(\"[WARNING] Could not extract coefficients for token analysis\")\n",
        "\n",
        "# Best FP\n",
        "if len(fp_idx) > 0:\n",
        "    best_fp = fp_idx[np.argmax(probs[fp_idx])]\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"HIGHEST CONFIDENCE FALSE POSITIVE (FP)\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"Index: {best_fp}\")\n",
        "    print(\"True label: 0 (not anxiety)\")\n",
        "    print(\"Predicted: 1 (anxiety)\")\n",
        "    print(f\"Probability: {probs[best_fp]:.4f}\")\n",
        "    print(\"\\nText excerpt (first 200 chars):\")\n",
        "    print(texts[best_fp][:200] + \"...\")\n",
        "\n",
        "    if coef is not None:\n",
        "        try:\n",
        "            x_tfidf = X_tfidf[best_fp]\n",
        "            contributions = x_tfidf.multiply(coef[: X_tfidf.shape[1]]).toarray()[0]\n",
        "            feature_names = vec.get_feature_names_out()\n",
        "            top_pos_idx = np.argsort(-contributions)[:8]\n",
        "            print(\"\\nTop anxiety-indicating tokens:\")\n",
        "            for idx in top_pos_idx:\n",
        "                if contributions[idx] > 0:\n",
        "                    print(f\"  '{feature_names[idx]}': +{contributions[idx]:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARNING] Could not extract token contributions: {e}\")\n",
        "\n",
        "# Best FN\n",
        "if len(fn_idx) > 0:\n",
        "    best_fn = fn_idx[np.argmin(probs[fn_idx])]\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"HIGHEST CONFIDENCE FALSE NEGATIVE (FN)\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"Index: {best_fn}\")\n",
        "    print(\"True label: 1 (anxiety)\")\n",
        "    print(\"Predicted: 0 (not anxiety)\")\n",
        "    print(f\"Probability: {probs[best_fn]:.4f}\")\n",
        "    print(\"\\nText excerpt (first 200 chars):\")\n",
        "    print(texts[best_fn][:200] + \"...\")\n",
        "\n",
        "    if coef is not None:\n",
        "        try:\n",
        "            x_tfidf = X_tfidf[best_fn]\n",
        "            contributions = x_tfidf.multiply(coef[: X_tfidf.shape[1]]).toarray()[0]\n",
        "            feature_names = vec.get_feature_names_out()\n",
        "            top_pos_idx = np.argsort(-contributions)[:8]\n",
        "            top_neg_idx = np.argsort(contributions)[:8]\n",
        "            print(\"\\nTop anxiety-indicating tokens:\")\n",
        "            for idx in top_pos_idx:\n",
        "                if contributions[idx] > 0:\n",
        "                    print(f\"  '{feature_names[idx]}': +{contributions[idx]:.4f}\")\n",
        "            print(\"\\nTop non-anxiety-indicating tokens:\")\n",
        "            for idx in top_neg_idx:\n",
        "                if contributions[idx] < 0:\n",
        "                    print(f\"  '{feature_names[idx]}': {contributions[idx]:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARNING] Could not extract token contributions: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
